{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "a_xHqE173acg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ldY0TDpP4Bw_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b43c78e"
      },
      "source": [
        "1. Environment Setup — Colab Mount, Paths, Assertions, Self-Test\n",
        "2. Imports, Reproducibility, Device Selection, Self-Check\n",
        "3. Training Shards — Discovery, Local Staging, Concatenation, Normalization Wrapper\n",
        "4. FER2013 CSV Splits — Validation/Test Datasets with Consistent Preprocessing\n",
        "5. DataLoaders and Optional Visualization Hook\n",
        "6. Metrics — Accuracy, Class Weights, and Composite Losses (Label Smoothing + Focal)\n",
        "7. MixUp and CutMix Utilities (+ Mixed-Criterion Wrapper)\n",
        "8. EMA (Weights) — Backup/Restore and Safety Checks\n",
        "9. Base Training/Evaluation Mixin (training_step/validation_step/aggregation)\n",
        "10. Training Hyperparameters and Global Knobs\n",
        "11. Model Definition — EfficientNet-B0 + CBAM + optional Sobel\n",
        "12. Optimizer & Scheduler (Warmup-Cosine) & EarlyStopping\n",
        "13. Training Loop — AMP, EMA, MixUp/CutMix, Save-Best Checkpoint\n",
        "14. Launch Training — Build Optimizer/Scheduler/EMA and Fit\n",
        "15. Evaluation Utilities — Base / EMA / EMA + TTA\n",
        "16. Run Evaluation — Validation and Test (Base, EMA, EMA+TTA)\n",
        "17. FLOPs Measurement (fvcore) for 96×96\n",
        "18. Efficiency Summary — Map Metrics, Select Best, Compute Accuracy/GFLOPs\n",
        "19. Save Final Checkpoint — Best Weights + Timestamped Copy\n",
        "20. Reload Checkpoint for Inference — Sanity Forward Pass\n",
        "21. FLOPs Calculation (fvcore) — Standalone Report Cell\n",
        "22. Efficiency Report Preparation — Name Mapping & Best Selection\n",
        "23. Efficiency Function — Accuracy(%) per GFLOP Utility\n",
        "24. Run Efficiency Report — Compute and Print Efficiency Score\n",
        "25. Save Final Checkpoint — Stage C/Final Model Artifact\n",
        "26. Wrap-Up — Final Metrics Summary and Completion Banner\n",
        "27. Stage A (AffectNet) — RGB, ImageNet Normalization, Training\n",
        "28. Stage A Save — Write AffectNet Checkpoint\n",
        "29. Stage B (RAF-DB) — Load Stage A, Fine-Tune on RAF-DB\n",
        "30. Stage B Save — Write RAF-DB Checkpoint\n",
        "31. Stage C (FER2013) — Load Stage B, Final Fine-Tune on FER2013\n",
        "32. Stage C Save — Write FER2013 Checkpoint (Multi-Stage Complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMZPNU-4fwT"
      },
      "source": [
        "Stage 0 — Boot & Storage Safety\n",
        "\n",
        "Cell 01 — Folder & Storage Debug (Auto-Create + Quota Warning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lod2dmJ5CAOk",
        "outputId": "f61e858e-0f51-4c49-8715-af0aecf3cb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] Disk @ /content/project — total=112.64 GB, used=39.13 GB, free=73.49 GB\n"
          ]
        }
      ],
      "source": [
        "# Creates core directories locally and prints disk quota.\n",
        "\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_NAME  = \"facial_expression_recognition_2025\"\n",
        "PROJECT_ROOT  = Path(\"./project\")\n",
        "DATA_ROOT     = Path(\"./data\")\n",
        "CKPT_DIR      = PROJECT_ROOT / \"checkpoints\"\n",
        "LOG_DIR       = PROJECT_ROOT / \"logs\"\n",
        "MIN_FREE_GB   = 2.0  # warn if below this free space\n",
        "\n",
        "for p in [PROJECT_ROOT, DATA_ROOT, CKPT_DIR, LOG_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "usage = shutil.disk_usage(str(PROJECT_ROOT))\n",
        "free_gb = usage.free / (1024**3)\n",
        "print(f\"[Stage0] Disk @ {PROJECT_ROOT.resolve()} — total={usage.total/(1024**3):.2f} GB, \"\n",
        "      f\"used={usage.used/(1024**3):.2f} GB, free={free_gb:.2f} GB\")\n",
        "if free_gb < MIN_FREE_GB:\n",
        "    print(f\"[Stage0][WARN] Low free space (< {MIN_FREE_GB:.1f} GB). Consider cleaning Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md70Tsg_M0Uj"
      },
      "source": [
        "#Cell 02 — Environment Setup (Colab Mount, Paths, Assertions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-CPW6ggM3uc",
        "outputId": "bf624417-ec17-4e66-f087-0265fdfe288a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] Drive unmounted.\n",
            "Mounted at /content/drive\n",
            "[Stage0] Google Drive mounted.\n",
            "[Stage0] FER CSV: /content/drive/MyDrive/fer2013.csv\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import drive\n",
        "\n",
        "    # First unmount if already mounted\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "        print(\"[Stage0] Drive unmounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Stage0] No previous mount or already unmounted. ({e})\")\n",
        "\n",
        "    # Now remount\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"[Stage0] Google Drive mounted.\")\n",
        "\n",
        "# Path to FER2013\n",
        "FER_CSV_PATH = Path(\"/content/drive/MyDrive/fer2013.csv\")\n",
        "print(f\"[Stage0] FER CSV: {FER_CSV_PATH if FER_CSV_PATH.exists() else 'NOT FOUND'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmnEFvxuM-OZ"
      },
      "source": [
        "#Cell 03 — Global Switches & Run Config (Single Source of Truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXqcOpdrNBci",
        "outputId": "dc19cb5e-f677-44d9-bf69-5a176d6d5062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] CONFIG snapshot:\n",
            "  - AUG_ALPHA       : 0.65\n",
            "  - AUG_CAP_LATE    : True\n",
            "  - BATCH_SIZE      : 192\n",
            "  - CKPT_DIR        : project/checkpoints\n",
            "  - DATA_ROOT       : data\n",
            "  - DRY_RUN         : False\n",
            "  - FER_CSV_PATH    : /content/drive/MyDrive/fer2013.csv\n",
            "  - IMG_SIZE        : 96\n",
            "  - LOG_DIR         : project/logs\n",
            "  - NUM_WORKERS     : 6\n",
            "  - PROJECT_ROOT    : project\n",
            "  - RUN_ALL         : False\n",
            "  - RUN_FER         : True\n",
            "  - RUN_STAGE_A     : False\n",
            "  - RUN_STAGE_B     : False\n",
            "  - RUN_STAGE_C     : False\n",
            "  - SAVE_BEST_PATH  : project/checkpoints/best_fer.pth\n",
            "  - SEED            : 42\n",
            "  - TAPER_MIX_LATE  : True\n",
            "  - USE_AUG         : True\n",
            "  - USE_AUG_ADV     : True\n",
            "  - USE_CUTMIX      : True\n",
            "  - USE_EMA         : True\n",
            "  - USE_MIXUP       : True\n",
            "  - USE_TTA         : False\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIG (single source of truth) — UPDATED ---\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure FER CSV path points to your Drive root\n",
        "FER_CSV_PATH = Path(\"/content/drive/MyDrive/fer2013.csv\")\n",
        "\n",
        "CONFIG = {\n",
        "    # === Feature toggles ===\n",
        "    \"USE_AUG\": True,          # enable training-time augmentation\n",
        "    \"USE_AUG_ADV\": True,      # advanced FER policy (AugMixLite + occlusion/elastic etc.)\n",
        "    \"AUG_ALPHA\": 0.65,        # blend coefficient for AugMixLite\n",
        "    \"USE_MIXUP\": True,\n",
        "    \"USE_CUTMIX\": True,\n",
        "    \"USE_EMA\": True,\n",
        "    \"USE_TTA\": False,         # keep Val clean; enable TTA only for Test in Cell 27\n",
        "\n",
        "    # === Late-phase controls ===\n",
        "    \"AUG_CAP_LATE\": True,     # cap augmentation strength in the last ~30% epochs\n",
        "    \"TAPER_MIX_LATE\": True,   # taper MixUp/CutMix late\n",
        "\n",
        "    # === Run routing ===\n",
        "    \"RUN_FER\": True,\n",
        "    \"RUN_STAGE_A\": False,\n",
        "    \"RUN_STAGE_B\": False,\n",
        "    \"RUN_STAGE_C\": False,\n",
        "    \"RUN_ALL\": False,\n",
        "    \"DRY_RUN\": False,\n",
        "\n",
        "    # === Dataloading & reproducibility (throughput tuned for Colab 83GB/40GB) ===\n",
        "    \"SEED\": 42,\n",
        "    \"NUM_WORKERS\": 6,         # try 6; if GPU still starves, try 8. If RAM spikes, drop to 4.\n",
        "    \"BATCH_SIZE\": 192,        # safe at 96x96 with 40GB GPU; if OOM, use 176/160/128\n",
        "    \"IMG_SIZE\": 96,\n",
        "\n",
        "    # === Paths ===\n",
        "    \"PROJECT_ROOT\": PROJECT_ROOT,\n",
        "    \"DATA_ROOT\": DATA_ROOT,\n",
        "    \"CKPT_DIR\": CKPT_DIR,\n",
        "    \"LOG_DIR\": LOG_DIR,\n",
        "    \"SAVE_BEST_PATH\": CKPT_DIR / \"best_fer.pth\",\n",
        "    \"FER_CSV_PATH\": FER_CSV_PATH,\n",
        "}\n",
        "\n",
        "# Nice, aligned snapshot\n",
        "print(\"[Stage0] CONFIG snapshot:\")\n",
        "for k in sorted(CONFIG.keys()):\n",
        "    print(f\"  - {k:16s}: {CONFIG[k]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfXgWu_NNDYj"
      },
      "source": [
        "#Cell 04 — Imports, Versions, Reproducibility, Device Self-Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JwHVOA2NGPf",
        "outputId": "4e01dedb-205a-4e70-b645-25eef2b348db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] torch=2.8.0+cu126, torchvision=0.23.0+cu126, numpy=2.0.2\n",
            "[Stage0] CUDA:0 — NVIDIA A100-SXM4-40GB — 39.6 GB VRAM\n"
          ]
        }
      ],
      "source": [
        "import math, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as VF\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(f\"[Stage0] torch={torch.__version__}, torchvision={torchvision.__version__}, numpy={np.__version__}\")\n",
        "\n",
        "SEED = int(CONFIG[\"SEED\"])\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    dev_id = torch.cuda.current_device()\n",
        "    props = torch.cuda.get_device_properties(dev_id)\n",
        "    print(f\"[Stage0] CUDA:{dev_id} — {props.name} — {props.total_memory/(1024**3):.1f} GB VRAM\")\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    print(\"[Stage0] Device: Apple MPS\")\n",
        "else:\n",
        "    print(\"[Stage0] Device: CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KDrvY2HNaqo"
      },
      "source": [
        "#Cell 05 — Training Shards: Discovery & Local Staging (No Normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPN-6d-CNddO",
        "outputId": "1955800b-53d0-4922-bb47-31db0cb073d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] No shards directory present (this is fine).\n"
          ]
        }
      ],
      "source": [
        "# Optional shard discovery; safe no-op if you do not pre-generate shards.\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "SHARDS_ROOT = CONFIG[\"DATA_ROOT\"] / \"augmented_data\"\n",
        "LOCAL_STAGE = CONFIG[\"DATA_ROOT\"] / \"_local_stage\"\n",
        "LOCAL_STAGE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if SHARDS_ROOT.exists():\n",
        "    shards = sorted([p for p in SHARDS_ROOT.rglob(\"*.pt\")])\n",
        "    print(f\"[Stage0] Found {len(shards)} shards under {SHARDS_ROOT}\")\n",
        "    if len(shards) > 0:\n",
        "        sample_dst = LOCAL_STAGE / shards[0].name\n",
        "        if not sample_dst.exists():\n",
        "            try:\n",
        "                shutil.copy2(shards[0], sample_dst)\n",
        "                print(f\"[Stage0] Staged sample shard → {sample_dst}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Stage0][WARN] Could not stage shard: {e}\")\n",
        "else:\n",
        "    print(\"[Stage0] No shards directory present (this is fine).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDnnfo0SNfpD"
      },
      "source": [
        "#Cell 06 — FER2013 CSV Parse & Split (Robust)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp2aJVzFNhaB",
        "outputId": "c78634bc-955d-47c2-ca0f-984d06110e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] FER splits — train=28709, val=3589, test=3589\n"
          ]
        }
      ],
      "source": [
        "# Loads FER2013 from the fixed path and makes train/val/test DataFrames.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "assert CONFIG[\"RUN_FER\"], \"RUN_FER=False; skip if not using FER2013.\"\n",
        "assert CONFIG[\"FER_CSV_PATH\"] is not None and Path(CONFIG[\"FER_CSV_PATH\"]).exists(), \\\n",
        "    \"FER2013 CSV not found at /content/drive/MyDrive/fer2013.csv\"\n",
        "\n",
        "fer_df = pd.read_csv(CONFIG[\"FER_CSV_PATH\"])\n",
        "expected_cols = {\"emotion\", \"pixels\"}\n",
        "assert expected_cols.issubset({c.lower() for c in fer_df.columns}), \\\n",
        "    f\"FER CSV missing required columns {expected_cols}. Found: {fer_df.columns.tolist()}\"\n",
        "\n",
        "if \"Usage\" in fer_df.columns:\n",
        "    tr_df = fer_df[fer_df[\"Usage\"] == \"Training\"].reset_index(drop=True)\n",
        "    va_df = fer_df[fer_df[\"Usage\"] == \"PublicTest\"].reset_index(drop=True)\n",
        "    te_df = fer_df[fer_df[\"Usage\"] == \"PrivateTest\"].reset_index(drop=True)\n",
        "else:\n",
        "    perm = np.random.permutation(len(fer_df))\n",
        "    n = len(fer_df); n_tr = int(0.8*n); n_va = int(0.1*n)\n",
        "    idx_tr, idx_va = perm[:n_tr], perm[n_tr:n_tr+n_va]\n",
        "    mask = np.zeros(n, dtype=bool); mask[idx_tr]=True; mask[idx_va]=True\n",
        "    tr_df = fer_df.iloc[idx_tr].reset_index(drop=True)\n",
        "    va_df = fer_df.iloc[idx_va].reset_index(drop=True)\n",
        "    te_df = fer_df.loc[~mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"[Stage0] FER splits — train={len(tr_df)}, val={len(va_df)}, test={len(te_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpSeNJeZNjCn"
      },
      "source": [
        "#Cell 07 — Dataset Definition (48×48 → 96×96, No Normalize Yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhid26vXNmQK",
        "outputId": "dbf9b755-c9ed-4315-dcfb-4a26dae426b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] Dataset objects built.\n"
          ]
        }
      ],
      "source": [
        "# CSV-backed dataset converting space-separated pixels → [1,H,W] float tensor.\n",
        "# Resize to 96×96 here; normalization is applied later (in aug or eval transform).\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df, img_size=96):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_size = int(img_size)\n",
        "        if len(self.df) > 0:\n",
        "            _ = self._row_to_tensor(0)\n",
        "\n",
        "    def _row_to_tensor(self, idx: int) -> torch.Tensor:\n",
        "        px = self.df.iloc[idx][\"pixels\"]\n",
        "        arr = np.fromstring(str(px), sep=\" \", dtype=np.float32)\n",
        "        assert arr.size == 48*48, f\"Row {idx}: expected 2304 pixels, got {arr.size}\"\n",
        "        img = torch.from_numpy(arr.reshape(48, 48)).unsqueeze(0)  # [1,48,48]\n",
        "        img = VF.resize(img, [self.img_size, self.img_size],\n",
        "                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
        "                        antialias=True)\n",
        "        return img  # still [0..255]\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self._row_to_tensor(idx)\n",
        "        y = int(self.df.iloc[idx][\"emotion\"])\n",
        "        return x, y\n",
        "\n",
        "IMG_SIZE = int(CONFIG[\"IMG_SIZE\"])\n",
        "train_ds = FER2013Dataset(tr_df, img_size=IMG_SIZE)\n",
        "valid_ds = FER2013Dataset(va_df, img_size=IMG_SIZE)\n",
        "test_ds  = FER2013Dataset(te_df, img_size=IMG_SIZE)\n",
        "print(\"[Stage0] Dataset objects built.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXSE5ctVNn0G"
      },
      "source": [
        "#Cell 08 — DataLoaders (No Aug Yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka5VEPz2Nr0-",
        "outputId": "b8529bdc-688f-4b81-ec9b-6a8032795ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage0] Train batch shape: torch.Size([192, 1, 96, 96]), torch.Size([192])\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE  = int(CONFIG[\"BATCH_SIZE\"])\n",
        "NUM_WORKERS = int(CONFIG[\"NUM_WORKERS\"])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                      persistent_workers=(NUM_WORKERS>0))\n",
        "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                      persistent_workers=(NUM_WORKERS>0))\n",
        "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2, shuffle=False,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                      persistent_workers=(NUM_WORKERS>0))\n",
        "\n",
        "xb, yb = next(iter(train_dl))\n",
        "print(f\"[Stage0] Train batch shape: {xb.shape}, {yb.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzRNS4RMOiay"
      },
      "source": [
        "#Cell 09 — Optional Visualization Hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HU_AbnxTOkKG"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_batch(dl, nrow=16, title='Raw FER2013 batch (resized)'):\n",
        "    imgs, labels = next(iter(dl))\n",
        "    grid = make_grid(imgs[:nrow], nrow=nrow)\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.axis('off'); plt.title(title)\n",
        "    plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment to preview\n",
        "#show_batch(train_dl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRvueO8vOvFy"
      },
      "source": [
        "#Cell 10 — Augmentation Primitives (Grayscale-Friendly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nTNv_xLoOxIF"
      },
      "outputs": [],
      "source": [
        "# ===== Advanced grayscale operators (tensor I/O) =====\n",
        "# INPUT  : x in [C=1,H,W], values in [0,255]\n",
        "# OUTPUT : same shape/range unless otherwise noted\n",
        "\n",
        "import math, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as VF\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "import io\n",
        "\n",
        "# ---- Small utilities ----\n",
        "def _to_pil_gray(x255: torch.Tensor) -> Image.Image:\n",
        "    # [1,H,W] -> PIL 'L'\n",
        "    x = x255.clamp(0, 255).to(torch.uint8).squeeze(0).cpu().numpy()\n",
        "    return Image.fromarray(x, mode='L')\n",
        "\n",
        "def _from_pil_gray(img: Image.Image) -> torch.Tensor:\n",
        "    # PIL 'L' -> [1,H,W] float32 in [0,255]\n",
        "    return torch.tensor(np.array(img, dtype=np.uint8), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "def _clamp255(x): return x.clamp(0.0, 255.0)\n",
        "\n",
        "# ---- Photometric / intensity ----\n",
        "def gauss_noise(x, sigma=0.02):\n",
        "    n = torch.randn_like(x) * (sigma * 255.0)\n",
        "    return _clamp255(x + n)\n",
        "\n",
        "def rand_gamma(x, gmin=0.8, gmax=1.25):\n",
        "    g = random.uniform(gmin, gmax)\n",
        "    x01 = (x / 255.0).clamp(0,1)\n",
        "    return (x01.pow(g) * 255.0)\n",
        "\n",
        "def rand_contrast(x, scale=0.25):\n",
        "    c = 1.0 + random.uniform(-scale, scale)\n",
        "    mean = x.mean(dim=(1,2), keepdim=True)\n",
        "    return _clamp255((x - mean) * c + mean)\n",
        "\n",
        "def rand_equalize(x):\n",
        "    img = _to_pil_gray(x); img = ImageOps.equalize(img)\n",
        "    return _from_pil_gray(img).to(x.dtype).to(x.device)\n",
        "\n",
        "def rand_jpeg(x, qmin=45, qmax=85):\n",
        "    img = _to_pil_gray(x)\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format='JPEG', quality=random.randint(qmin, qmax))\n",
        "    buf.seek(0)\n",
        "    img2 = Image.open(buf).convert('L')\n",
        "    return _from_pil_gray(img2).to(x.dtype).to(x.device)\n",
        "\n",
        "def rand_vignette(x, strength=0.25):\n",
        "    _, H, W = x.shape\n",
        "    yy, xx = torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                            torch.linspace(-1,1,W,device=x.device), indexing='ij')\n",
        "    r = torch.sqrt(xx**2 + yy**2)\n",
        "    mask = 1.0 - strength * (r / r.max()).clamp(0,1)\n",
        "    return _clamp255(x * mask.unsqueeze(0))\n",
        "\n",
        "def rand_blur(x, k=3):\n",
        "    return VF.gaussian_blur(x, kernel_size=k)\n",
        "\n",
        "# ---- Geometric ----\n",
        "def rand_affine_small(x, max_rot=12.0, max_trans=0.08, max_shear=8.0, max_scale=0.08):\n",
        "    H, W = x.shape[-2:]\n",
        "    angle = random.uniform(-max_rot, max_rot)\n",
        "    trans = [int(random.uniform(-max_trans, max_trans) * W),\n",
        "             int(random.uniform(-max_trans, max_trans) * H)]\n",
        "    scale = 1.0 + random.uniform(-max_scale, max_scale)\n",
        "    shear = [random.uniform(-max_shear, max_shear), 0.0]\n",
        "    return VF.affine(x, angle=angle, translate=trans, scale=scale, shear=shear)\n",
        "\n",
        "def rand_pad_crop(x, pad=3):\n",
        "    # pad then random crop back to original size\n",
        "    _, H, W = x.shape\n",
        "    xpad = F.pad(x, (pad, pad, pad, pad), mode='reflect')\n",
        "    i = random.randint(0, 2*pad); j = random.randint(0, 2*pad)\n",
        "    return xpad[:, i:i+H, j:j+W]\n",
        "\n",
        "def rand_hflip(x, p=0.5):\n",
        "    return VF.hflip(x) if random.random() < p else x\n",
        "\n",
        "# Elastic deformation: small, smoothed displacement field\n",
        "def rand_elastic(x, alpha=1.0, sigma=4.0):\n",
        "    _, H, W = x.shape\n",
        "    # displacement fields\n",
        "    dx = torch.randn(1,1,H,W, device=x.device)\n",
        "    dy = torch.randn(1,1,H,W, device=x.device)\n",
        "    # smooth them by Gaussian\n",
        "    def _gauss_kernel(k=21, s=sigma):\n",
        "        ax = torch.arange(k, device=x.device) - (k-1)/2\n",
        "        ker = torch.exp(-(ax**2)/(2*s*s)); ker = ker/ker.sum()\n",
        "        return ker\n",
        "    k = 21\n",
        "    gx = _gauss_kernel(k).view(1,1,1,k)\n",
        "    gy = _gauss_kernel(k).view(1,1,k,1)\n",
        "    dx = F.conv2d(dx, gx, padding=(0,k//2)); dx = F.conv2d(dx, gy, padding=(k//2,0))\n",
        "    dy = F.conv2d(dy, gx, padding=(0,k//2)); dy = F.conv2d(dy, gy, padding=(k//2,0))\n",
        "    dx = dx.squeeze(0).squeeze(0) * alpha\n",
        "    dy = dy.squeeze(0).squeeze(0) * alpha\n",
        "\n",
        "    # grid in [-1,1]\n",
        "    yy, xx = torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                            torch.linspace(-1,1,W,device=x.device), indexing='ij')\n",
        "    xx = (xx + dx / (W/2)).clamp(-1,1)\n",
        "    yy = (yy + dy / (H/2)).clamp(-1,1)\n",
        "    grid = torch.stack([xx, yy], dim=-1).unsqueeze(0)  # [1,H,W,2]\n",
        "    return F.grid_sample(x.unsqueeze(0), grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(0)\n",
        "\n",
        "# ---- Occlusion (domain-specific) ----\n",
        "def band_occlusion(x, mode='eyes', frac=0.18):\n",
        "    _, H, W = x.shape\n",
        "    band_h = max(1, int(frac*H))\n",
        "    y0 = {\n",
        "        'eyes': int(0.35*H) - band_h//2,\n",
        "        'mouth': int(0.75*H) - band_h//2,\n",
        "        'top': int(0.15*H) - band_h//2\n",
        "    }[mode]\n",
        "    y1 = max(0, y0); y2 = min(H, y0 + band_h)\n",
        "    x = x.clone()\n",
        "    x[:, y1:y2, :] = x[:, y1:y2, :].mean()  # neutral occluder (gray)\n",
        "    return x\n",
        "\n",
        "def localized_erasing(x, min_frac=0.01, max_frac=0.05):\n",
        "    C, H, W = x.shape\n",
        "    area = random.uniform(min_frac, max_frac) * H * W\n",
        "    side = int(max(2, math.sqrt(area)))\n",
        "    cx, cy = random.randint(0,W-1), random.randint(0,H-1)\n",
        "    x[:, max(0,cy-side//2):min(H,cy+side//2), max(0,cx-side//2):min(W,cx+side//2)] = 127.5\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3wTzPMWOygO"
      },
      "source": [
        "#Cell 11 — Augmentation Pipeline Builder (Curriculum; Output Normalized [-1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cxNGpw7nO1MI"
      },
      "outputs": [],
      "source": [
        "# ===== AugMixLite: blend multiple sub-policies with the original =====\n",
        "def _apply_bank(x, bank, k=2):\n",
        "    y = x.clone()\n",
        "    for _ in range(k):\n",
        "        op = random.choice(bank)\n",
        "        y = op(y)\n",
        "    return y\n",
        "\n",
        "def augmix_lite(x, banks, alpha=0.65, branches=2, depth=2):\n",
        "    mix = x.clone()\n",
        "    for _ in range(branches):\n",
        "        b = random.choice(banks)\n",
        "        y = _apply_bank(x, b, k=depth)\n",
        "        mix = mix + y\n",
        "    mix = mix / (branches + 1.0)\n",
        "    return (1 - alpha) * x + alpha * mix\n",
        "\n",
        "# ===== Advanced augmentation builder =====\n",
        "def build_advanced_fer_augment(strength: float):\n",
        "    \"\"\"\n",
        "    strength s in [0,1]: controls probabilities/magnitudes.\n",
        "    Returns f(x255->[1,H,W]) -> x_norm in [-1,1].\n",
        "    \"\"\"\n",
        "    s = float(max(0.0, min(1.0, strength)))\n",
        "    # Probabilities\n",
        "    p_photo = 0.7 * (0.5 + 0.5*s)\n",
        "    p_geom  = 0.6 * (0.5 + 0.5*s)\n",
        "    p_occl  = 0.40 * (0.5 + 0.5*s)\n",
        "    p_equal = 0.20 * s\n",
        "    p_blur  = 0.15 * s\n",
        "    # Magnitudes\n",
        "    gamma_rng = (0.85 - 0.15*s, 1.20 + 0.05*s)\n",
        "    contrast_scale = 0.20 + 0.10*s\n",
        "    jpeg_q = (55 - int(10*s), 85)\n",
        "    vignette_str = 0.15 + 0.20*s\n",
        "    elastic_alpha = 0.6 + 0.8*s\n",
        "    rot = 10 + 5*s\n",
        "    shear = 6 + 4*s\n",
        "    trans = 0.06 + 0.03*s\n",
        "    scale = 0.06 + 0.04*s\n",
        "\n",
        "    # Banks\n",
        "    photometric_bank = [\n",
        "        lambda z: gauss_noise(z, sigma=0.015 + 0.02*s),\n",
        "        lambda z: rand_gamma(z, *gamma_rng),\n",
        "        lambda z: rand_contrast(z, scale=contrast_scale),\n",
        "        lambda z: rand_jpeg(z, qmin=jpeg_q[0], qmax=jpeg_q[1]),\n",
        "        lambda z: rand_vignette(z, strength=vignette_str),\n",
        "    ]\n",
        "    geometric_bank = [\n",
        "        lambda z: rand_affine_small(z, max_rot=rot, max_trans=trans, max_shear=shear, max_scale=scale),\n",
        "        lambda z: rand_pad_crop(z, pad=3),\n",
        "        lambda z: rand_hflip(z, p=0.5),\n",
        "        lambda z: rand_elastic(z, alpha=elastic_alpha, sigma=4.0),\n",
        "    ]\n",
        "    occlusion_bank = [\n",
        "        lambda z: band_occlusion(z, mode='eyes',  frac=0.16 + 0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='mouth', frac=0.16 + 0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='top',   frac=0.14 + 0.06*s),\n",
        "        lambda z: localized_erasing(z, min_frac=0.01, max_frac=0.05),\n",
        "    ]\n",
        "    banks = [photometric_bank, geometric_bank, occlusion_bank]\n",
        "\n",
        "    def _norm_to_m11(x255):\n",
        "        x01 = (x255 / 255.0).clamp(0,1)\n",
        "        return (x01 - 0.5) * 2.0\n",
        "\n",
        "    def _augment(x):\n",
        "        # 1) pre-crop/pad + optional blur\n",
        "        if random.random() < p_geom:  x = rand_pad_crop(x, pad=3)\n",
        "        if random.random() < p_blur:  x = rand_blur(x, k=3)\n",
        "\n",
        "        # 2) photometric block\n",
        "        if random.random() < p_photo: x = random.choice(photometric_bank)(x)\n",
        "\n",
        "        # 3) AugMixLite composite (2 branches × depth=2)\n",
        "        x = augmix_lite(x, banks=banks, alpha=CONFIG.get(\"AUG_ALPHA\", 0.65),\n",
        "                        branches=2, depth=2)\n",
        "\n",
        "        # 4) more geometric and occlusion chance\n",
        "        if random.random() < p_geom:  x = random.choice(geometric_bank)(x)\n",
        "        if random.random() < p_occl:  x = random.choice(occlusion_bank)(x)\n",
        "\n",
        "        # 5) occasional histogram equalization near the end\n",
        "        if random.random() < p_equal: x = rand_equalize(x)\n",
        "\n",
        "        # Normalize to [-1,1] for the model\n",
        "        return _norm_to_m11(x)\n",
        "\n",
        "    return _augment\n",
        "\n",
        "# Keep legacy factory for fallback\n",
        "FER_AUG_FACTORY = build_advanced_fer_augment if CONFIG.get(\"USE_AUG_ADV\", False) else build_fer_augment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQFbYZsO3nL"
      },
      "source": [
        "#Cell 12 — Augmentation Debug Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XEOf68KO5Sp",
        "outputId": "734abd5b-36c6-4678-c92e-c25af5ef3678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AugDebug] s=0.0: min=-1.000, max=0.978\n",
            "[AugDebug] s=0.5: min=-1.000, max=0.910\n",
            "[AugDebug] s=1.0: min=-1.000, max=1.000\n"
          ]
        }
      ],
      "source": [
        "if CONFIG[\"USE_AUG\"]:\n",
        "    xs, ys = next(iter(train_dl))\n",
        "    xs = xs[:8].to('cpu')\n",
        "    for s in [0.0, 0.5, 1.0]:\n",
        "        f = FER_AUG_FACTORY(s)\n",
        "        x_aug = torch.stack([f(x) for x in xs])  # [-1,1]\n",
        "        assert x_aug.shape == xs.shape and torch.isfinite(x_aug).all()\n",
        "        print(f\"[AugDebug] s={s}: min={x_aug.min().item():.3f}, max={x_aug.max().item():.3f}\")\n",
        "        # Optional quick glance\n",
        "        # grid = (x_aug * 0.5 + 0.5).clamp(0,1)\n",
        "        # grid = make_grid(grid, nrow=8)\n",
        "        # plt.figure(figsize=(12,3)); plt.axis('off'); plt.title(f'Advanced Aug s={s}')\n",
        "        # plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray'); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4ZRABSbO7ip"
      },
      "source": [
        "#Cell 13 — Metrics: Accuracy & Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0cCKIM2O-El",
        "outputId": "1de2bde4-1cef-450c-9a53-f719be424e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1] Class weights: [0.4800022542476654, 4.398185729980469, 0.4680519998073578, 0.26578086614608765, 0.39702051877975464, 0.6047332286834717, 0.3862254023551941]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def accuracy(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    return (logits.argmax(dim=1) == targets).float().mean()\n",
        "\n",
        "def compute_class_weights(df) -> torch.Tensor:\n",
        "    counts = Counter(int(e) for e in df[\"emotion\"].tolist())\n",
        "    total = sum(counts.values())\n",
        "    weights = [total / max(1, counts.get(c, 1)) for c in range(7)]\n",
        "    w = torch.tensor(weights, dtype=torch.float32)\n",
        "    return w / w.mean()\n",
        "\n",
        "CLASS_WEIGHTS = compute_class_weights(tr_df)\n",
        "print(f\"[Stage1] Class weights: {CLASS_WEIGHTS.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFH8-w0iPAra"
      },
      "source": [
        "#Cell 14 — Losses: Label-Smoothed CE, Focal, Composite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bD7TSaSDPCpd"
      },
      "outputs": [],
      "source": [
        "# Replace your CompositeLoss with this “SmoothedFocal” preset.\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.10, reduction='mean'):\n",
        "        super().__init__(); self.eps=eps; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(-1)\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true = torch.zeros_like(logp).fill_(self.eps/(n-1))\n",
        "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps)\n",
        "        loss = -(true * logp).sum(dim=1)\n",
        "        return loss.mean() if self.reduction=='mean' else loss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.5, reduction='mean'):\n",
        "        super().__init__(); self.g=gamma; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        fl = ((1-pt)**self.g) * ce\n",
        "        return fl.mean() if self.reduction=='mean' else fl\n",
        "\n",
        "class SmoothedFocal(nn.Module):\n",
        "    def __init__(self, eps=0.10, gamma=1.5, alpha=0.70, weight=None):\n",
        "        super().__init__(); self.a=alpha; self.w = weight\n",
        "        self.lsce = LabelSmoothingCE(eps)\n",
        "        self.focal= FocalLoss(gamma)\n",
        "    def forward(self, logits, targets):\n",
        "        if self.w is not None:\n",
        "            # weight affects CE inside focal; apply by hand\n",
        "            ce = F.cross_entropy(logits, targets, reduction='none', weight=self.w.to(logits.device))\n",
        "            pt = torch.exp(-ce); fl = ((1-pt)**1.5) * ce\n",
        "            ls = self.lsce(logits, targets)\n",
        "            return self.a*ls + (1-self.a)*fl.mean()\n",
        "        return self.a*self.lsce(logits, targets) + (1-self.a)*self.focal(logits, targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZvGD2yRPD-B"
      },
      "source": [
        "#Cell 15 — MixUp & CutMix Utilities + Mixed Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "V_GdzW7CPG1_"
      },
      "outputs": [],
      "source": [
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0.0:\n",
        "        return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0), device=x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, (y_a, y_b), lam, index\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0, min_lam=0.3, max_lam=0.7):\n",
        "    if alpha <= 0.0:\n",
        "        return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    lam = float(max(min_lam, min(max_lam, lam)))\n",
        "    B, C, H, W = x.size()\n",
        "    index = torch.randperm(B, device=x.device)\n",
        "    cut_w = int(W * math.sqrt(1 - lam))\n",
        "    cut_h = int(H * math.sqrt(1 - lam))\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H + 1e-9))\n",
        "    y_a, y_b = y, y[index]\n",
        "    return x, (y_a, y_b), lam, index\n",
        "\n",
        "def mixed_criterion(criterion, logits, targets_mix, lam):\n",
        "    if isinstance(targets_mix, tuple):\n",
        "        y_a, y_b = targets_mix\n",
        "        return lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n",
        "    else:\n",
        "        return criterion(logits, targets_mix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikjEOYF6PMce"
      },
      "source": [
        "#Cell 16 — EMA (Exponential Moving Average) for Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Aux44u9ePPhv"
      },
      "outputs": [],
      "source": [
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
        "        self.decay = float(decay)\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, model: nn.Module):\n",
        "        for name, param in model.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "\n",
        "    def apply_shadow(self, model: nn.Module):\n",
        "        self.backup = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            self.backup[name] = param.data.clone()\n",
        "            param.data = self.shadow[name].clone()\n",
        "\n",
        "    def restore(self, model: nn.Module):\n",
        "        for name, param in model.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            param.data = self.backup[name].clone()\n",
        "        self.backup = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jrmj1e_PQwd"
      },
      "source": [
        "#Cell 17 — Base Training/Evaluation Mixin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nknmYGNnPTGM"
      },
      "outputs": [],
      "source": [
        "class TrainingMixin:\n",
        "    def training_step(self, batch, criterion):\n",
        "        x, y = batch\n",
        "        x, y = x.to(self.device), y.to(self.device)\n",
        "        logits = self(x)\n",
        "        loss = criterion(logits, y)\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_step(self, batch, criterion):\n",
        "        x, y = batch\n",
        "        x, y = x.to(self.device), y.to(self.device)\n",
        "        logits = self(x)\n",
        "        loss = criterion(logits, y)\n",
        "        acc = accuracy(logits, y)\n",
        "        return loss.detach(), acc.detach()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_loader(self, loader, criterion):\n",
        "        self.eval()\n",
        "        losses, accs = [], []\n",
        "        for batch in loader:\n",
        "            l, a = self.validation_step(batch, criterion)\n",
        "            losses.append(l.item()); accs.append(a.item())\n",
        "        return float(np.mean(losses)), float(np.mean(accs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ymRPHafPUZs"
      },
      "source": [
        "#Cell 18 — Training Hyperparameters & Global Knobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oUfKVzEPW9Z",
        "outputId": "d4b903af-0ca3-4d8a-a88d-917bd1a1d6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1] HP snapshot: {'EPOCHS': 70, 'LR': 0.0003, 'WD': 5e-05, 'WARMUP_EPOCHS': 4, 'LR_MIN': 5e-05, 'PATIENCE': 18, 'EMA_DECAY': 0.9995, 'MIXUP_ALPHA': 0.3, 'CUTMIX_ALPHA': 1.0, 'AUG_RAMP_EPOCHS': 0.25}\n"
          ]
        }
      ],
      "source": [
        "# === Squeezed HP (late-improvement friendly) ===\n",
        "HP = {\n",
        "    \"EPOCHS\": 70,          # give the cosine tail more room\n",
        "    \"LR\": 3e-4,            # good starting LR for AdamW fine-tune\n",
        "    \"WD\": 5e-5,            # was 1e-4; slightly lower to reduce underfit in the tail\n",
        "    \"WARMUP_EPOCHS\": 4,    # keep warmup\n",
        "    \"LR_MIN\": 5e-5,        # raise tail floor so updates don’t die out (you were still learning at ~3e-5)\n",
        "    \"PATIENCE\": 18,        # was 8; your run was still improving at ep40 → avoid premature stop\n",
        "    \"EMA_DECAY\": 0.9995,   # a touch slower EMA helps late stability\n",
        "    \"MIXUP_ALPHA\": 0.3,\n",
        "    \"CUTMIX_ALPHA\": 1.0,\n",
        "    \"AUG_RAMP_EPOCHS\": 0.25, # ramp a bit faster; we’ll taper later anyway\n",
        "}\n",
        "print(\"[Stage1] HP snapshot:\", HP)\n",
        "\n",
        "# --- Late-phase tweaks (outside HP; keep your existing fit(...) call) ---\n",
        "# Use these variables if your training loop supports them; otherwise just apply the logic in code.\n",
        "P_MIX, P_CUT = 0.30, 0.30           # keep some mixing early\n",
        "TAPER_START_FRAC, TAPER_END_FRAC = 0.60, 0.90   # linearly taper mix/aug to ~0 by last 10%\n",
        "LABEL_SMOOTH_EPS = 0.05             # feed into your loss if supported (CrossEntropy w/ label smoothing)\n",
        "\n",
        "# Example (conceptual) inside your epoch loop:\n",
        "# frac = epoch / HP[\"EPOCHS\"]\n",
        "# p_mix_now = P_MIX if frac < TAPER_START_FRAC else max(0.0, P_MIX * (1 - (frac - TAPER_START_FRAC) / (TAPER_END_FRAC - TAPER_START_FRAC)))\n",
        "# p_cut_now = P_CUT if frac < TAPER_START_FRAC else max(0.0, P_CUT * (1 - (frac - TAPER_START_FRAC) / (TAPER_END_FRAC - TAPER_START_FRAC)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0E74WzYPYh3"
      },
      "source": [
        "#Cell 19 — CBAM Block (Attention Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "eUopqauqPbdt"
      },
      "outputs": [],
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch, r=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(ch, max(1, ch//r), 1, bias=True), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1, ch//r), ch, 1, bias=True)\n",
        "        )\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        ca = F.adaptive_avg_pool2d(x, 1) + F.adaptive_max_pool2d(x, 1)\n",
        "        ca = self.sigmoid(self.mlp(ca))\n",
        "        x = x * ca\n",
        "        ms = torch.cat([x.mean(1, keepdim=True), x.max(1, keepdim=True)[0]], dim=1)\n",
        "        sa = self.spatial(ms)\n",
        "        return x * sa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp3IswFBPcx7"
      },
      "source": [
        "#Cell 20 — Optional Sobel Stem (for Grayscale Edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "s6AxJfsjPe89"
      },
      "outputs": [],
      "source": [
        "class SobelStem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kernel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        kernel_y = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
        "        self.register_buffer('kx', kernel_x.view(1,1,3,3))\n",
        "        self.register_buffer('ky', kernel_y.view(1,1,3,3))\n",
        "    def forward(self, x):\n",
        "        gx = F.conv2d(x, self.kx, padding=1)\n",
        "        gy = F.conv2d(x, self.ky, padding=1)\n",
        "        g = torch.sqrt(gx**2 + gy**2 + 1e-6)\n",
        "        return torch.cat([x, g], dim=1)  # [B,2,H,W]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER92xzPAPga2"
      },
      "source": [
        "#Cell 21 — HybridEffNet Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Tvrj2UVTPkkw"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 21 — HybridEffNet Model Definition (REPLACE) =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "# Pull optional knobs from CONFIG with safe defaults\n",
        "CLASSIFIER_DROPOUT = float(CONFIG.get(\"CLASSIFIER_DROPOUT\", 0.30))\n",
        "USE_CBAM = bool(CONFIG.get(\"USE_CBAM\", True))  # keep True to match prior runs\n",
        "\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
        "        w  = torch.stack([kx, ky]).unsqueeze(1)  # (2,1,3,3)\n",
        "        self.register_buffer('w', w)\n",
        "\n",
        "    def forward(self, x):            # x:[B,1,H,W]\n",
        "        edges = F.conv2d(x, self.w, padding=1)   # [B,2,H,W]\n",
        "        return torch.cat([x, edges], dim=1)      # [B,3,H,W]\n",
        "\n",
        "class HybridEffNet(nn.Module, TrainingMixin):\n",
        "    def __init__(self, num_classes=7, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        base = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "\n",
        "        # Sobel expands 1→3 channels so we can keep EfficientNet stem unchanged\n",
        "        self.sobel    = SobelLayer()\n",
        "        self.features = base.features\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Optional CBAM on the final feature map (requires Cell 19)\n",
        "        self.cbam = CBAM(1280) if use_cbam else None\n",
        "\n",
        "        in_features = 1280  # EfficientNet-B0 penultimate dim\n",
        "        self.bn   = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=classifier_dropout)\n",
        "        self.head = nn.Linear(in_features, num_classes)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x1):           # x1 in [-1,1], shape [B,1,H,W]\n",
        "        x3 = self.sobel(x1)          # [B,3,H,W]\n",
        "        f  = self.features(x3)       # [B,1280,h,w]\n",
        "        if self.cbam is not None:\n",
        "            f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1) # [B,1280]\n",
        "        f  = self.bn(f)\n",
        "        f  = self.drop(f)\n",
        "        return self.head(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU_ywDDUPmB4"
      },
      "source": [
        "#Cell 22 — Optimizer, Warmup-Cosine Scheduler, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "mYHbUtgmPn9I"
      },
      "outputs": [],
      "source": [
        "def make_adamw(params, lr, wd):\n",
        "    return torch.optim.AdamW(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, warmup_epochs, max_epochs, lr_min=1e-6, lr_max=None):\n",
        "        self.opt = optimizer\n",
        "        self.warmup = max(1, int(warmup_epochs))\n",
        "        self.maxe = int(max_epochs)\n",
        "        self.t = 0\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max if lr_max is not None else max(g['lr'] for g in optimizer.param_groups)\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        if self.t <= self.warmup:\n",
        "            lr = self.lr_min + (self.lr_max - self.lr_min) * (self.t / self.warmup)\n",
        "        else:\n",
        "            tt = (self.t - self.warmup) / max(1, (self.maxe - self.warmup))\n",
        "            lr = self.lr_min + 0.5*(self.lr_max - self.lr_min)*(1 + math.cos(math.pi*tt))\n",
        "        for g in self.opt.param_groups:\n",
        "            g['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=8, min_delta=1e-4):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.best = float('inf')\n",
        "        self.bad = 0\n",
        "    def step(self, val_loss):\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.bad = 0\n",
        "            return False\n",
        "        self.bad += 1\n",
        "        return self.bad >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z9WyhD3PpUd"
      },
      "source": [
        "#Cell 23 — fit_with_aug(): Training Loop with Aug/Mix/EMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "SlOTFD_yPrxa"
      },
      "outputs": [],
      "source": [
        "# === Cell 23 — fit_with_aug(): advanced aug + late-phase taper + AMP + EMA ===\n",
        "def fit_with_aug(model: nn.Module, train_dl, valid_dl, hp, config, on_epoch_end=None):\n",
        "    device = model.device\n",
        "\n",
        "    # Loss with class weights (computed earlier)\n",
        "    weight   = CLASS_WEIGHTS.to(device)\n",
        "    criterion = SmoothedFocal(eps=0.10, gamma=1.5, alpha=0.70, weight=weight)\n",
        "\n",
        "    # Optimizer / Scheduler / EarlyStop / EMA / AMP\n",
        "    opt   = make_adamw(model.parameters(), lr=hp[\"LR\"], wd=hp[\"WD\"])\n",
        "    sched = WarmupCosine(opt,\n",
        "                         warmup_epochs=hp[\"WARMUP_EPOCHS\"],\n",
        "                         max_epochs=hp[\"EPOCHS\"], lr_min=hp[\"LR_MIN\"])\n",
        "    stopper = EarlyStopping(patience=hp[\"PATIENCE\"], min_delta=1e-4)\n",
        "    ema     = EMA(model, decay=hp[\"EMA_DECAY\"]) if config[\"USE_EMA\"] else None\n",
        "    scaler  = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    # Curriculum ramp for augmentation strength\n",
        "    total_epochs  = int(hp[\"EPOCHS\"])\n",
        "    aug_ramp_frac = float(hp.get(\"AUG_RAMP_EPOCHS\", 0.30))\n",
        "    ramp_epochs   = max(1, int(aug_ramp_frac * total_epochs))\n",
        "\n",
        "    # Feature flags for late-phase controls\n",
        "    cap_late   = bool(config.get(\"AUG_CAP_LATE\", True))\n",
        "    taper_late = bool(config.get(\"TAPER_MIX_LATE\", True))\n",
        "\n",
        "    history = []\n",
        "    best_val = float(\"inf\")  # track best val_loss for reference/logging\n",
        "\n",
        "    for epoch in range(1, total_epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss_sum = 0.0\n",
        "        num_seen = 0\n",
        "\n",
        "        # ----- Build augmentation for this epoch -----\n",
        "        if config[\"USE_AUG\"]:\n",
        "            # curriculum strength in [0.2, 0.8]\n",
        "            s = 0.2 + 0.6 * min(1.0, epoch / ramp_epochs)\n",
        "            # optional cap in the final 30% to better match val distribution\n",
        "            if cap_late and epoch >= int(0.7 * total_epochs):\n",
        "                s = min(s, 0.6)\n",
        "            augment = FER_AUG_FACTORY(s)\n",
        "        else:\n",
        "            augment = None\n",
        "\n",
        "        # ----- Late-phase taper for mixing (MixUp/CutMix) -----\n",
        "        mixup_alpha  = float(hp[\"MIXUP_ALPHA\"])\n",
        "        cutmix_alpha = float(hp[\"CUTMIX_ALPHA\"])\n",
        "        use_cutmix   = bool(config[\"USE_CUTMIX\"])\n",
        "\n",
        "        if taper_late and epoch >= int(0.5 * total_epochs):\n",
        "            mixup_alpha  = max(0.1, mixup_alpha * 0.5)   # softer mixing\n",
        "            cutmix_alpha = max(0.5, cutmix_alpha * 0.5)  # smaller boxes\n",
        "        if taper_late and epoch >= int(0.7 * total_epochs):\n",
        "            use_cutmix = False  # sharpen decision boundaries late\n",
        "\n",
        "        # ===================== Train loop =====================\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "            # Apply augmentation or deterministic normalization\n",
        "            if augment is not None:\n",
        "                # advanced policy returns tensors already normalized to [-1,1]\n",
        "                xb = torch.stack([augment(x) for x in xb])  # [-1,1]\n",
        "            else:\n",
        "                xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16,\n",
        "                                enabled=torch.cuda.is_available()):\n",
        "                if config[\"USE_MIXUP\"] or use_cutmix:\n",
        "                    if use_cutmix and random.random() < 0.5:\n",
        "                        xb, targets_mix, lam, _ = cutmix_data(\n",
        "                            xb, yb, alpha=cutmix_alpha, min_lam=0.3, max_lam=0.7\n",
        "                        )\n",
        "                    else:\n",
        "                        xb, targets_mix, lam, _ = mixup_data(xb, yb, alpha=mixup_alpha)\n",
        "                    logits = model(xb)\n",
        "                    loss   = mixed_criterion(criterion, logits, targets_mix, lam)\n",
        "                else:\n",
        "                    logits = model(xb)\n",
        "                    loss   = criterion(logits, yb)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "            if ema is not None:\n",
        "                ema.update(model)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            epoch_loss_sum += float(loss.item()) * bs\n",
        "            num_seen += bs\n",
        "\n",
        "        # ===================== Validation =====================\n",
        "        @torch.no_grad()\n",
        "        def _eval(loader):\n",
        "            model.eval()\n",
        "            losses, accs = [], []\n",
        "            for xb, yb in loader:\n",
        "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "                xb = ((xb / 255.0) - 0.5) * 2.0  # eval is augmentation-free\n",
        "                logits = model(xb)\n",
        "                l = criterion(logits, yb)\n",
        "                a = accuracy(logits, yb)\n",
        "                losses.append(l.item()); accs.append(a.item())\n",
        "            return float(np.mean(losses)), float(np.mean(accs))\n",
        "\n",
        "        val_loss, val_acc = _eval(valid_dl)\n",
        "\n",
        "        # Scheduler step after seeing validation\n",
        "        lr_now = sched.step()\n",
        "\n",
        "        # Epoch bookkeeping\n",
        "        train_loss = epoch_loss_sum / max(1, num_seen)\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": float(train_loss),\n",
        "            \"val_loss\":   float(val_loss),\n",
        "            \"val_acc\":    float(val_acc),\n",
        "            \"lr\":         float(lr_now),\n",
        "        })\n",
        "\n",
        "        # >>> NEW: call external hook (e.g., to save best-by-acc) <<<\n",
        "        if callable(on_epoch_end):\n",
        "            try:\n",
        "                on_epoch_end(epoch, float(val_loss), float(val_acc))\n",
        "            except Exception as e:\n",
        "                print(f\"[warn] on_epoch_end hook failed at epoch {epoch}: {e}\")\n",
        "\n",
        "        # Logging with best marker on val_loss (early stop follows this)\n",
        "        is_best = \"\"\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            is_best = \" *best*\"\n",
        "        print(f\"[Epoch {epoch:03d}/{total_epochs}] \"\n",
        "              f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
        "              f\"val_acc={val_acc:.4f}  lr={lr_now:.2e}{is_best}\")\n",
        "\n",
        "        # Early stopping on validation loss (after hook so the final epoch can be saved)\n",
        "        if stopper.step(val_loss):\n",
        "            print(\"[EarlyStopping] Patience exhausted; stopping training.\")\n",
        "            break\n",
        "\n",
        "    return history, ema  # ema is your EMA object (or None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KE1e7ExPtTP"
      },
      "source": [
        "#Cell 24 — Integration Debug: One Forward/Backward Probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4_cgoJ2PwbQ",
        "outputId": "2ee8e8e8-5164-4901-c0b7-9e622d6bad8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Initialized new model.\n",
            "[Probe] model params ≈ 4.43M, device=cuda:0\n",
            "[Probe] logits=(192, 7), loss=2.1656, grad_head_norm=3.032\n"
          ]
        }
      ],
      "source": [
        "# ===== Build or reuse the model, and optionally load best checkpoint =====\n",
        "from pathlib import Path\n",
        "import torch, torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BEST_ACC_PATH = Path(\"/content/drive/MyDrive/fer_runs/checkpoints/best_by_acc.pth\")\n",
        "\n",
        "def _make_model():\n",
        "    m = HybridEffNet(num_classes=7, classifier_dropout=0.30, use_cbam=True)\n",
        "    m.to(device)\n",
        "    return m\n",
        "\n",
        "# 1) Do NOT re-initialize if a model already exists in memory\n",
        "if \"model\" in globals() and isinstance(model, HybridEffNet):\n",
        "    print(\"[Probe] Re-using existing model in memory.\")\n",
        "else:\n",
        "    model = _make_model()\n",
        "    print(\"[Probe] Initialized new model.\")\n",
        "\n",
        "# 2) Optionally load best-by-acc checkpoint if present (for resume/eval)\n",
        "if BEST_ACC_PATH.exists():\n",
        "    ck = torch.load(BEST_ACC_PATH, map_location=\"cpu\")\n",
        "    model.load_state_dict(ck[\"model_state\"])\n",
        "    model.to(device)\n",
        "    print(f\"[Probe] Loaded checkpoint: epoch={ck.get('epoch')}, \"\n",
        "          f\"val_acc={ck['metrics'].get('val_acc'):.4f}\")\n",
        "\n",
        "model.train()  # set to train mode for the probe\n",
        "\n",
        "# ---- Quick param count banner ----\n",
        "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "print(f\"[Probe] model params ≈ {total_params:.2f}M, device={next(model.parameters()).device}\")\n",
        "\n",
        "# ---- One mini-batch probe (sanity on shapes/loss/gradients) ----\n",
        "xb, yb = next(iter(train_dl))\n",
        "xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "# Deterministic normalization for the probe (matches your eval path)\n",
        "xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
        "    logits = model(xb)\n",
        "    loss   = F.cross_entropy(logits, yb)\n",
        "\n",
        "loss.backward()\n",
        "head_grad_norm = (getattr(model, \"head\").weight.grad.norm().item()\n",
        "                  if hasattr(model, \"head\") else float(\"nan\"))\n",
        "print(f\"[Probe] logits={tuple(logits.shape)}, loss={loss.item():.4f}, grad_head_norm={head_grad_norm:.3f}\")\n",
        "\n",
        "# Optional: clear grads after probe to start training cleanly\n",
        "model.zero_grad(set_to_none=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CljeZ8RPyPk"
      },
      "source": [
        "#Cell 25 — Launch Training (FER-only Path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Checkpoint Saver (Best by val_acc) — updated & robust\n",
        "# ======================\n",
        "from pathlib import Path\n",
        "import time, os, torch\n",
        "\n",
        "# Directory for checkpoints\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/fer_runs/checkpoints\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_ACC_PATH = CKPT_DIR / \"best_by_acc.pth\"\n",
        "\n",
        "def _atomic_save(state: dict, path: Path):\n",
        "    \"\"\"Write-and-rename to avoid partial files on networked FS (e.g., Drive).\"\"\"\n",
        "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
        "    torch.save(state, tmp)\n",
        "    os.replace(tmp, path)  # atomic on POSIX\n",
        "\n",
        "class CheckpointSaverAccOnly:\n",
        "    \"\"\"\n",
        "    Minimal, picklable saver that stores only the *best by val_acc*.\n",
        "    Call: saver.on_epoch_end(epoch, val_loss, val_acc)\n",
        "    \"\"\"\n",
        "    def __init__(self, model, ema=None, cfg=None, hp=None, meta=None):\n",
        "        self.model, self.ema = model, ema\n",
        "        self.cfg, self.hp    = cfg or {}, hp or {}\n",
        "        self.meta            = meta or {}\n",
        "        self.best_acc        = 0.0\n",
        "\n",
        "        # Resume running \"best\" if a prior best exists (nice for interrupted runs)\n",
        "        if BEST_ACC_PATH.exists():\n",
        "            try:\n",
        "                ck = torch.load(BEST_ACC_PATH, map_location=\"cpu\")\n",
        "                prev = float(ck.get(\"metrics\", {}).get(\"val_acc\", 0.0))\n",
        "                self.best_acc = max(self.best_acc, prev)\n",
        "                print(f\"[Saver] Resuming best_acc threshold from disk: {self.best_acc:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Saver] Could not read prior best: {e}\")\n",
        "\n",
        "    def _pack(self, epoch, metrics: dict):\n",
        "        return {\n",
        "            \"epoch\": int(epoch),\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"metrics\": {k: float(v) for k, v in metrics.items()},\n",
        "            \"config\": self.cfg, \"hp\": self.hp, \"meta\": self.meta,\n",
        "            \"classes\": getattr(train_ds, \"classes\", None),\n",
        "            \"class_to_idx\": getattr(train_ds, \"class_to_idx\", None),\n",
        "            \"model_state\": self.model.state_dict(),\n",
        "            \"ema_state\": (self.ema.state_dict() if self.ema and hasattr(self.ema, \"state_dict\") else None),\n",
        "        }\n",
        "\n",
        "    def on_epoch_end(self, epoch, val_loss, val_acc):\n",
        "        # compare on raw float (caller should pass floats already)\n",
        "        if float(val_acc) > self.best_acc + 1e-6:\n",
        "            self.best_acc = float(val_acc)\n",
        "            state = self._pack(epoch, {\"val_loss\": float(val_loss), \"val_acc\": self.best_acc})\n",
        "\n",
        "            # Stable artifact\n",
        "            _atomic_save(state, BEST_ACC_PATH)\n",
        "            # Timestamped backup (optional but handy)\n",
        "            stamped = CKPT_DIR / f\"best_by_acc_{time.strftime('%Y%m%d_%H%M%S')}.pth\"\n",
        "            _atomic_save(state, stamped)\n",
        "\n",
        "            print(f\"[Save] New best val_acc={self.best_acc:.4f} → {BEST_ACC_PATH.name} (+ backup {stamped.name})\")\n"
      ],
      "metadata": {
        "id": "KAAmZKqmBeAf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpsSwTOYP0-d",
        "outputId": "7a2da54c-2622-4414-eb04-51ddcf4e5680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage3] Starting training…\n",
            "[Save] New best val_acc=0.4506 → best_by_acc.pth (+ backup best_by_acc_20250820_103957.pth)\n",
            "[Epoch 001/70] train_loss=1.4763  val_loss=1.2285  val_acc=0.4506  lr=1.12e-04 *best*\n",
            "[Save] New best val_acc=0.4867 → best_by_acc.pth (+ backup best_by_acc_20250820_104041.pth)\n",
            "[Epoch 002/70] train_loss=1.3703  val_loss=1.1638  val_acc=0.4867  lr=1.75e-04 *best*\n",
            "[Save] New best val_acc=0.5229 → best_by_acc.pth (+ backup best_by_acc_20250820_104127.pth)\n",
            "[Epoch 003/70] train_loss=1.3286  val_loss=1.1130  val_acc=0.5229  lr=2.37e-04 *best*\n",
            "[Save] New best val_acc=0.5462 → best_by_acc.pth (+ backup best_by_acc_20250820_104212.pth)\n",
            "[Epoch 004/70] train_loss=1.3095  val_loss=1.0740  val_acc=0.5462  lr=3.00e-04 *best*\n",
            "[Save] New best val_acc=0.5633 → best_by_acc.pth (+ backup best_by_acc_20250820_104258.pth)\n",
            "[Epoch 005/70] train_loss=1.2607  val_loss=1.0314  val_acc=0.5633  lr=3.00e-04 *best*\n",
            "[Save] New best val_acc=0.5782 → best_by_acc.pth (+ backup best_by_acc_20250820_104345.pth)\n",
            "[Epoch 006/70] train_loss=1.2619  val_loss=1.0262  val_acc=0.5782  lr=2.99e-04 *best*\n",
            "[Save] New best val_acc=0.5967 → best_by_acc.pth (+ backup best_by_acc_20250820_104431.pth)\n",
            "[Epoch 007/70] train_loss=1.2251  val_loss=0.9901  val_acc=0.5967  lr=2.99e-04 *best*\n",
            "[Save] New best val_acc=0.6066 → best_by_acc.pth (+ backup best_by_acc_20250820_104518.pth)\n",
            "[Epoch 008/70] train_loss=1.2309  val_loss=0.9807  val_acc=0.6066  lr=2.98e-04 *best*\n",
            "[Save] New best val_acc=0.6149 → best_by_acc.pth (+ backup best_by_acc_20250820_104605.pth)\n",
            "[Epoch 009/70] train_loss=1.1971  val_loss=0.9548  val_acc=0.6149  lr=2.96e-04 *best*\n",
            "[Save] New best val_acc=0.6227 → best_by_acc.pth (+ backup best_by_acc_20250820_104653.pth)\n",
            "[Epoch 010/70] train_loss=1.1863  val_loss=0.9519  val_acc=0.6227  lr=2.95e-04 *best*\n",
            "[Save] New best val_acc=0.6333 → best_by_acc.pth (+ backup best_by_acc_20250820_104740.pth)\n",
            "[Epoch 011/70] train_loss=1.1606  val_loss=0.9327  val_acc=0.6333  lr=2.93e-04 *best*\n",
            "[Save] New best val_acc=0.6338 → best_by_acc.pth (+ backup best_by_acc_20250820_104828.pth)\n",
            "[Epoch 012/70] train_loss=1.1735  val_loss=0.9444  val_acc=0.6338  lr=2.91e-04\n",
            "[Save] New best val_acc=0.6419 → best_by_acc.pth (+ backup best_by_acc_20250820_104917.pth)\n",
            "[Epoch 013/70] train_loss=1.1680  val_loss=0.9543  val_acc=0.6419  lr=2.89e-04\n",
            "[Save] New best val_acc=0.6458 → best_by_acc.pth (+ backup best_by_acc_20250820_105006.pth)\n",
            "[Epoch 014/70] train_loss=1.1460  val_loss=0.9300  val_acc=0.6458  lr=2.86e-04 *best*\n",
            "[Save] New best val_acc=0.6482 → best_by_acc.pth (+ backup best_by_acc_20250820_105055.pth)\n",
            "[Epoch 015/70] train_loss=1.1495  val_loss=0.9110  val_acc=0.6482  lr=2.83e-04 *best*\n",
            "[Save] New best val_acc=0.6511 → best_by_acc.pth (+ backup best_by_acc_20250820_105145.pth)\n",
            "[Epoch 016/70] train_loss=1.1363  val_loss=0.9118  val_acc=0.6511  lr=2.80e-04\n",
            "[Save] New best val_acc=0.6577 → best_by_acc.pth (+ backup best_by_acc_20250820_105235.pth)\n",
            "[Epoch 017/70] train_loss=1.1489  val_loss=0.9070  val_acc=0.6577  lr=2.77e-04 *best*\n",
            "[Epoch 018/70] train_loss=1.1228  val_loss=0.9258  val_acc=0.6535  lr=2.73e-04\n",
            "[Save] New best val_acc=0.6634 → best_by_acc.pth (+ backup best_by_acc_20250820_105414.pth)\n",
            "[Epoch 019/70] train_loss=1.1495  val_loss=0.9068  val_acc=0.6634  lr=2.69e-04 *best*\n",
            "[Save] New best val_acc=0.6649 → best_by_acc.pth (+ backup best_by_acc_20250820_105504.pth)\n",
            "[Epoch 020/70] train_loss=1.1385  val_loss=0.9064  val_acc=0.6649  lr=2.65e-04 *best*\n",
            "[Save] New best val_acc=0.6688 → best_by_acc.pth (+ backup best_by_acc_20250820_105553.pth)\n",
            "[Epoch 021/70] train_loss=1.1241  val_loss=0.8930  val_acc=0.6688  lr=2.61e-04 *best*\n",
            "[Save] New best val_acc=0.6718 → best_by_acc.pth (+ backup best_by_acc_20250820_105643.pth)\n",
            "[Epoch 022/70] train_loss=1.0895  val_loss=0.8881  val_acc=0.6718  lr=2.57e-04 *best*\n",
            "[Save] New best val_acc=0.6780 → best_by_acc.pth (+ backup best_by_acc_20250820_105734.pth)\n",
            "[Epoch 023/70] train_loss=1.1166  val_loss=0.8823  val_acc=0.6780  lr=2.52e-04 *best*\n",
            "[Save] New best val_acc=0.6799 → best_by_acc.pth (+ backup best_by_acc_20250820_105824.pth)\n",
            "[Epoch 024/70] train_loss=1.1024  val_loss=0.8886  val_acc=0.6799  lr=2.48e-04\n",
            "[Epoch 025/70] train_loss=1.1112  val_loss=0.8976  val_acc=0.6769  lr=2.43e-04\n",
            "[Epoch 026/70] train_loss=1.1225  val_loss=0.8966  val_acc=0.6779  lr=2.37e-04\n",
            "[Save] New best val_acc=0.6837 → best_by_acc.pth (+ backup best_by_acc_20250820_110054.pth)\n",
            "[Epoch 027/70] train_loss=1.0682  val_loss=0.8811  val_acc=0.6837  lr=2.32e-04 *best*\n",
            "[Epoch 028/70] train_loss=1.0996  val_loss=0.8895  val_acc=0.6816  lr=2.27e-04\n",
            "[Epoch 029/70] train_loss=1.0828  val_loss=0.8880  val_acc=0.6756  lr=2.21e-04\n",
            "[Save] New best val_acc=0.6844 → best_by_acc.pth (+ backup best_by_acc_20250820_110324.pth)\n",
            "[Epoch 030/70] train_loss=1.0970  val_loss=0.8755  val_acc=0.6844  lr=2.16e-04 *best*\n",
            "[Save] New best val_acc=0.6870 → best_by_acc.pth (+ backup best_by_acc_20250820_110414.pth)\n",
            "[Epoch 031/70] train_loss=1.0618  val_loss=0.8875  val_acc=0.6870  lr=2.10e-04\n",
            "[Save] New best val_acc=0.6883 → best_by_acc.pth (+ backup best_by_acc_20250820_110505.pth)\n",
            "[Epoch 032/70] train_loss=1.0381  val_loss=0.8698  val_acc=0.6883  lr=2.04e-04 *best*\n",
            "[Save] New best val_acc=0.6921 → best_by_acc.pth (+ backup best_by_acc_20250820_110555.pth)\n",
            "[Epoch 033/70] train_loss=1.0454  val_loss=0.8639  val_acc=0.6921  lr=1.99e-04 *best*\n",
            "[Epoch 034/70] train_loss=1.0412  val_loss=0.8767  val_acc=0.6839  lr=1.93e-04\n",
            "[Save] New best val_acc=0.6932 → best_by_acc.pth (+ backup best_by_acc_20250820_110736.pth)\n",
            "[Epoch 035/70] train_loss=1.0112  val_loss=0.8697  val_acc=0.6932  lr=1.87e-04\n",
            "[Epoch 036/70] train_loss=1.0008  val_loss=0.8649  val_acc=0.6917  lr=1.81e-04\n",
            "[Save] New best val_acc=0.6943 → best_by_acc.pth (+ backup best_by_acc_20250820_110917.pth)\n",
            "[Epoch 037/70] train_loss=0.9886  val_loss=0.8594  val_acc=0.6943  lr=1.75e-04 *best*\n",
            "[Save] New best val_acc=0.6979 → best_by_acc.pth (+ backup best_by_acc_20250820_111007.pth)\n",
            "[Epoch 038/70] train_loss=0.9485  val_loss=0.8605  val_acc=0.6979  lr=1.69e-04\n",
            "[Save] New best val_acc=0.6994 → best_by_acc.pth (+ backup best_by_acc_20250820_111058.pth)\n",
            "[Epoch 039/70] train_loss=1.0194  val_loss=0.8697  val_acc=0.6994  lr=1.63e-04\n",
            "[Epoch 040/70] train_loss=0.9956  val_loss=0.8690  val_acc=0.6862  lr=1.57e-04\n",
            "[Epoch 041/70] train_loss=0.9793  val_loss=0.8683  val_acc=0.6870  lr=1.51e-04\n",
            "[Epoch 042/70] train_loss=0.9458  val_loss=0.8671  val_acc=0.6924  lr=1.46e-04\n",
            "[Epoch 043/70] train_loss=0.9587  val_loss=0.8616  val_acc=0.6943  lr=1.40e-04\n",
            "[Epoch 044/70] train_loss=0.9862  val_loss=0.8736  val_acc=0.6937  lr=1.34e-04\n",
            "[Epoch 045/70] train_loss=0.9471  val_loss=0.8711  val_acc=0.6982  lr=1.29e-04\n",
            "[Save] New best val_acc=0.7004 → best_by_acc.pth (+ backup best_by_acc_20250820_111650.pth)\n",
            "[Epoch 046/70] train_loss=0.9787  val_loss=0.8651  val_acc=0.7004  lr=1.23e-04\n",
            "[Epoch 047/70] train_loss=0.9471  val_loss=0.8659  val_acc=0.6972  lr=1.18e-04\n",
            "[Epoch 048/70] train_loss=0.9515  val_loss=0.8684  val_acc=0.6992  lr=1.13e-04\n",
            "[Epoch 049/70] train_loss=0.6925  val_loss=0.8873  val_acc=0.6964  lr=1.07e-04\n",
            "[Epoch 050/70] train_loss=0.7469  val_loss=0.8860  val_acc=0.6907  lr=1.02e-04\n",
            "[Epoch 051/70] train_loss=0.6981  val_loss=0.8802  val_acc=0.6952  lr=9.77e-05\n",
            "[Epoch 052/70] train_loss=0.6960  val_loss=0.8829  val_acc=0.6936  lr=9.31e-05\n",
            "[Epoch 053/70] train_loss=0.6850  val_loss=0.8902  val_acc=0.6994  lr=8.87e-05\n",
            "[Epoch 054/70] train_loss=0.7088  val_loss=0.8968  val_acc=0.6935  lr=8.45e-05\n",
            "[Epoch 055/70] train_loss=0.7051  val_loss=0.8847  val_acc=0.6959  lr=8.05e-05\n",
            "[EarlyStopping] Patience exhausted; stopping training.\n"
          ]
        }
      ],
      "source": [
        "if CONFIG[\"RUN_FER\"] and not CONFIG[\"DRY_RUN\"]:\n",
        "    print(\"[Stage3] Starting training…\")\n",
        "    saver = CheckpointSaverAccOnly(model, ema=None, cfg=CONFIG, hp=HP)\n",
        "    history, ema_obj = fit_with_aug(model, train_dl, valid_dl, HP, CONFIG,\n",
        "                                    on_epoch_end=saver.on_epoch_end)  # hook here\n",
        "else:\n",
        "    print(\"[Stage3] Skipping training due to DRY_RUN or RUN_FER=False.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJFgErAOP4-u"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}